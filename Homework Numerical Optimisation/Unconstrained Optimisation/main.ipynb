{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import steepest_descent_bcktrck as sdb\n",
    "from steepest_descent_bcktrck import *\n",
    "import cgm_pol_rib as cgmpb\n",
    "from cgm_pol_rib import *\n",
    "import functions as funcs\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'cgm_pol_rib' from '/home/claudio/Documents/GitHub/Homework-Numerical-Optimisation-/Homework Numerical Optimisation/Unconstrained Optimisation/cgm_pol_rib.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(funcs)\n",
    "reload(sdb)\n",
    "reload(cgmpb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate rate of convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_of_convergence(xseq: np.ndarray) ->np.ndarray:\n",
    "    N = 100\n",
    "    n = xseq.shape[1]\n",
    "    p = np.empty(N)\n",
    "    for k in range(1, N+1):\n",
    "        if (xseq[:, k+2]-xseq[:, k+1]).any() != 0 and (xseq[:, k+1]-xseq[:, k]).any() != 0 and (xseq[:, k]-xseq[:, k-1]).any() != 0:\n",
    "            p[k-1] = log(np.linalg.norm(xseq[:, k+2]-xseq[:, k+1], 2) / (np.linalg.norm(xseq[:, k+1]-xseq[:, k], 2))) / \\\n",
    "                     log(np.linalg.norm(xseq[:, k+1]-xseq[:, k], 2) / (np.linalg.norm(xseq[:, k]-xseq[:, k-1], 2)))\n",
    "            # print(f\"i = {k-1}: p = {p}\")\n",
    "    return p"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialisation of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.array([1.2, 1.2]).reshape(-1,1)\n",
    "x1 = np.array([-1.2, 1]).reshape(-1,1)\n",
    "alpha0 = 1\n",
    "tolgrad = 1e-12\n",
    "rho = 0.5\n",
    "c = 1e-4\n",
    "kmax = 50000\n",
    "btmax = 50\n",
    "fin_diff = True\n",
    "fd_type = 'centered'\n",
    "\n",
    "params = {\"c\": [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6],\n",
    "              \"rho\": [0.2, 0.3, 0.4, 0.5, 0.6, 0.7]}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the Steepest descent and conjugate gradient method with Backtrack tuning the parameters c and rho using the Armijo condition to see what is the result obtained (POINT X0)\n",
    "\n",
    "Here our idea is quite simple. In order to make a complete comparison between the two methods the best set of paramenters for both methods is evaluated, once this is done a comparison is made with the best result obtained in one of the two methods with one set and the same set is also used for the other method (we will therefore have in all 2 saved evaluations for steepest descent, one with the best set of parameters, and the other with the best set of parameters of the conjugate gradient method, and 2 saved evaluations for the conjugate gradient method) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best result for steepest descent method is saved here\n",
    "sd_best_fk_x0 = 1\n",
    "sd_best_param_x0 = 0\n",
    "sd_best_k_x0 = 0\n",
    "sd_best_gradfk_norm_x0 = 10\n",
    "sd_best_xk_x0 = np.empty((2,1))\n",
    "\n",
    "#best result for conjugate gradient method is saved here\n",
    "cg_best_fk_x0 = 1\n",
    "cg_best_param_x0 = 0\n",
    "cg_best_k_x0 = 0\n",
    "cg_best_gradfk_norm_x0 = 10\n",
    "cg_best_xk_x0 = np.empty((2,1))\n",
    "\n",
    "#Other result of steepest descent method is saved here\n",
    "sd_fk_x0 = 1\n",
    "sd_param_x0 = 0\n",
    "sd_k_x0 = 0\n",
    "sd_xk_x0 = np.empty((2,1))\n",
    "\n",
    "#Other result of conjugate gradient method is saved here\n",
    "cg_fk_x0 = 1\n",
    "cg_param_x0 = 0\n",
    "cg_k_x0 = 0\n",
    "cg__xk_x0 = np.empty((2,1))\n",
    "\n",
    "for param in ParameterGrid(params): \n",
    "    sd_xk, sd_fk, sd_gradfk_norm, sd_k, sd_xseq, sd_btseq = steepest_descent_bcktrck(x0, 'Rosenbrock', alpha0, kmax, tolgrad, param[\"c\"], param[\"rho\"], btmax, fin_diff, fd_type)\n",
    "\n",
    "    cg_xk, cg_fk, cg_gradfk_norm, cg_k, cg_xseq, cg_btseq = cgm_pol_rib(x0, 'Rosenbrock', alpha0, kmax, tolgrad, param[\"c\"], param[\"rho\"], btmax, fin_diff, fd_type)\n",
    "    \n",
    "    if (sd_fk < sd_best_fk_x0): \n",
    "        sd_best_param_x0 = param\n",
    "\n",
    "        sd_best_xk_x0 = sd_xk\n",
    "        sd_best_k_x0 = sd_k\n",
    "        sd_best_fk_x0 = sd_fk\n",
    "        sd_best_gradfk_norm_x0 = sd_gradfk_norm\n",
    "        sd_best_xseq_x0 = np.copy(sd_xseq)\n",
    "        \n",
    "        cg_xk_x0 = cg_xk\n",
    "        cg_k_x0 = cg_k\n",
    "        cg_fk_x0 = cg_fk\n",
    "            \n",
    "    if (cg_fk < cg_best_fk_x0): \n",
    "        cg_best_param_x0 = param\n",
    "\n",
    "        cg_best_xk_x0 = cg_xk\n",
    "        cg_best_k_x0 = cg_k\n",
    "        cg_best_fk_x0 = cg_fk\n",
    "        cg_best_gradfk_norm_x0 = cg_gradfk_norm\n",
    "        cg_best_xseq_x0 = np.copy(cg_xseq)\n",
    "\n",
    "        \n",
    "        sd_xk_x0 = sd_xk\n",
    "        sd_k_x0 = sd_k\n",
    "        sd_fk_x0 = sd_fk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation with steepest descent method:\n",
      "x0:  [1.2] [1.2]\n",
      "best set of parameters:  {'c': 0.01, 'rho': 0.7}\n",
      "xk:  [1.] [1.]\n",
      "k:  31839\n",
      "fk:  [4.17569518e-25]\n",
      "\n",
      "Evaluation of the conjugate gradient method using the best parameters of steepest descent: \n",
      "x0:  [1.2] [1.2]\n",
      "set of parameters:  0\n",
      "xk:  [1.] [1.]\n",
      "k:  8589\n",
      "fk:  [3.1537594e-25]\n"
     ]
    }
   ],
   "source": [
    "print(\"Best evaluation with steepest descent method:\")\n",
    "print(\"x0: \", x0[0], x0[1])\n",
    "print(\"best set of parameters: \", sd_best_param_x0)\n",
    "print(\"xk: \", sd_best_xk_x0[0], sd_best_xk_x0[1])\n",
    "print(\"k: \", sd_best_k_x0)\n",
    "print(\"fk: \", sd_best_fk_x0) \n",
    "\n",
    "print(\"\\nEvaluation of the conjugate gradient method using the best parameters of steepest descent: \")\n",
    "print(\"x0: \", x0[0], x0[1])\n",
    "print(\"set of parameters: \", sd_best_param_x0)\n",
    "print(\"xk: \", cg_xk_x0[0], cg_xk_x0[1])\n",
    "print(\"k: \", cg_k_x0)\n",
    "print(\"fk: \", cg_fk_x0) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best evaluation with steepest descent method:\n",
    "\n",
    "x0:  [1.2] [1.2]\n",
    "\n",
    "best set of parameters:  {'c': 0.01, 'rho': 0.7}\n",
    "\n",
    "xk:  [1.] [1.]\n",
    "\n",
    "k:  31839\n",
    "\n",
    "fk:  [4.17569518e-25]\n",
    "\n",
    "\n",
    "Evaluation of the conjugate gradient method using the best parameters of steepest descent: \n",
    "\n",
    "x0:  [1.2] [1.2]\n",
    "\n",
    "set of parameters:  {'c': 0.01, 'rho': 0.7}\n",
    "\n",
    "xk:  [1.] [1.]\n",
    "\n",
    "k:  8589\n",
    "\n",
    "fk:  [3.1537594e-25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation with steepest descent method:\n",
      "x0:  [1.2] [1.2]\n",
      "best set of parameters:  {'c': 0.1, 'rho': 0.4}\n",
      "xk:  [1.] [1.]\n",
      "k:  330\n",
      "fk:  [8.72184338e-29]\n",
      "\n",
      "Evaluation of the conjugate gradient method using the best parameters of steepest descent: \n",
      "x0:  [1.2] [1.2]\n",
      "set of parameters:  0\n",
      "xk:  [1.] [1.]\n",
      "k:  27922\n",
      "fk:  [8.37329555e-25]\n"
     ]
    }
   ],
   "source": [
    "print(\"Best evaluation with conjugate gradient method:\")\n",
    "print(\"x0: \", x0[0], x0[1])\n",
    "print(\"best set of parameters: \", cg_best_param_x0)\n",
    "print(\"xk: \", cg_best_xk_x0[0], cg_best_xk_x0[1])\n",
    "print(\"k: \", cg_best_k_x0)\n",
    "print(\"fk: \", cg_best_fk_x0) \n",
    "\n",
    "print(\"\\nEvaluation of the steepest descent method using the best parameters of conjugate gradient: \")\n",
    "print(\"x0: \", x0[0], x0[1])\n",
    "print(\"set of parameters: \", cg_best_param_x0)\n",
    "print(\"xk: \", sd_xk_x0[0], sd_xk_x0[1])\n",
    "print(\"k: \", sd_k_x0)\n",
    "print(\"fk: \", sd_fk_x0) \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best evaluation with conjugate gradient method:\n",
    "\n",
    "x0:  [1.2] [1.2]\n",
    "\n",
    "best set of parameters:  {'c': 0.1, 'rho': 0.4}\n",
    "\n",
    "xk:  [1.] [1.]\n",
    "\n",
    "k:  330\n",
    "\n",
    "fk:  [8.72184338e-29]\n",
    "\n",
    "Evaluation of the steepest descent method using the best parameters of conjugate gradient: \n",
    "\n",
    "x0:  [1.2] [1.2]\n",
    "\n",
    "set of parameters:  {'c': 0.1, 'rho': 0.4}\n",
    "\n",
    "xk:  [1.] [1.]\n",
    "\n",
    "k:  27922\n",
    "\n",
    "fk:  [8.37329555e-25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of rate of convergence for steepest descent method:\n",
      "p mean = 1.13\n"
     ]
    }
   ],
   "source": [
    "if (sd_best_gradfk_norm_x0 <= tolgrad):\n",
    "    #It make sense to evaluate the rate of convergence if and only if I've reached the solution \n",
    "    p = rate_of_convergence(sd_best_xseq_x0)\n",
    "    print(\"Evaluation of rate of convergence for steepest descent method:\")\n",
    "    print(f\"p mean = {np.abs(p).mean():.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of rate of convergence for steepest descent method:\n",
    "\n",
    "p mean = 1.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of rate of convergence for conjugate gradient method:\n",
      "p mean = 3.78\n"
     ]
    }
   ],
   "source": [
    "if (cg_best_gradfk_norm_x0 <= tolgrad):\n",
    "    #It make sense to evaluate the rate of convergence if and only if I've reached the solution \n",
    "    p = rate_of_convergence(cg_best_xseq_x0)\n",
    "    print(\"Evaluation of rate of convergence for conjugate gradient method:\")\n",
    "    print(f\"p mean = {np.abs(p).mean():.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of rate of convergence for conjugate gradient method:\n",
    "\n",
    "p mean = 3.78"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do the same but using another starting point (x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best result for steepest descent method is saved here\n",
    "sd_best_fk_x1 = 1\n",
    "sd_best_param_x1 = 0\n",
    "sd_best_k_x1 = 0\n",
    "sd_best_gradfk_norm_x1 = 10\n",
    "sd_best_xk_x1 = np.empty((2,1))\n",
    "\n",
    "#best result for conjugate gradient method is saved here\n",
    "cg_best_fk_x1 = 1\n",
    "cg_best_param_x1 = 0\n",
    "cg_best_k_x1 = 0\n",
    "cg_best_gradfk_norm_x1 = 10\n",
    "cg_best_xk_x1 = np.empty((2,1)if (sd_best_gradfk_norm_x0 <= tolgrad):\n",
    "    #It make sense to evaluate the rate of convergence if and only if I've reached the solution \n",
    "    p = rate_of_convergence(sd_best_xseq_x0)\n",
    "    print(\"Evaluation of rate of convergence for steepest descent method:\")\n",
    "    print(f\"p mean = {np.abs(p).mean():.2f}\"))\n",
    "\n",
    "#Other result of steepest descent method is saved here\n",
    "sd_fk_x1 = 1\n",
    "sd_param_x1 = 0\n",
    "sd_k_x1 = 0\n",
    "sd_xk_x1 = np.empty((2,1))\n",
    "\n",
    "#Other result of conjugate gradient method is saved here\n",
    "cg_fk_x1 = 1\n",
    "cg_param_x1 = 0\n",
    "cg_k_x1 = 0\n",
    "cg__xk_x1 = np.empty((2,1))\n",
    "\n",
    "for param in ParameterGrid(params): \n",
    "    sd_xk, sd_fk, sd_gradfk_norm, sd_k, sd_xseq, sd_btseq = steepest_descent_bcktrck(x1, 'Rosenbrock', alpha0, kmax, tolgrad, param[\"c\"], param[\"rho\"], btmax, fin_diff, fd_type)\n",
    "\n",
    "    cg_xk, cg_fk, cg_gradfk_norm, cg_k, cg_xseq, cg_btseq = cgm_pol_rib(x1, 'Rosenbrock', alpha0, kmax, tolgrad, param[\"c\"], param[\"rho\"], btmax, fin_diff, fd_type)\n",
    "    \n",
    "    if (sd_fk < sd_best_fk_x0): \n",
    "        sd_best_param_x1 = param\n",
    "\n",
    "        sd_best_xk_x1 = sd_xk\n",
    "        sd_best_k_x1 = sd_k\n",
    "        sd_best_fk_x1 = sd_fk\n",
    "        sd_best_gradfk_norm_x1 = sd_gradfk_norm\n",
    "        sd_best_xseq_x1 = np.copy(sd_xseq)\n",
    "        \n",
    "        cg_xk_x1 = cg_xk\n",
    "        cg_k_x1 = cg_k\n",
    "        cg_fk_x1 = cg_fk\n",
    "            \n",
    "    if (cg_fk < cg_best_fk_x1): \n",
    "        cg_best_param_x1 = param\n",
    "\n",
    "        cg_best_xk_x1 = cg_xk\n",
    "        cg_best_k_x1 = cg_k\n",
    "        cg_best_fk_x1 = cg_fk\n",
    "        cg_best_gradfk_norm_x1 = cg_gradfk_norm\n",
    "        cg_best_xseq_x1 = np.copy(cg_xseq)\n",
    "\n",
    "        \n",
    "        sd_xk_x1 = sd_xk\n",
    "        sd_k_x1 = sd_k\n",
    "        sd_fk_x1 = sd_fk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation with steepest descent method:\n",
      "x1:  [-1.2] [1.]\n",
      "best set of parameters:  {'c': 1e-06, 'rho': 0.7}\n",
      "xk:  [1.] [1.]\n",
      "k:  35285\n",
      "fk:  [4.17283211e-25]\n",
      "\n",
      "Evaluation of the conjugate gradient method using the best parameters of steepest descent: \n",
      "x0:  [-1.2] [1.]\n",
      "set of parameters:  {'c': 1e-06, 'rho': 0.7}\n",
      "xk:  [1.] [1.]\n",
      "k:  8191\n",
      "fk:  [2.04761125e-25]\n"
     ]
    }
   ],
   "source": [
    "print(\"Best evaluation with steepest descent method:\")\n",
    "print(\"x1: \", x1[0], x1[1])\n",
    "print(\"best set of parameters: \", sd_best_param_x1)\n",
    "print(\"xk: \", sd_best_xk_x1[0], sd_best_xk_x1[1])\n",
    "print(\"k: \", sd_best_k_x1)\n",
    "print(\"fk: \", sd_best_fk_x1) \n",
    "\n",
    "print(\"\\nEvaluation of the conjugate gradient method using the best parameters of steepest descent: \")\n",
    "print(\"x0: \", x1[0], x1[1])\n",
    "print(\"set of parameters: \", sd_best_param_x1)\n",
    "print(\"xk: \", cg_xk_x1[0], cg_xk_x1[1])\n",
    "print(\"k: \", cg_k_x1)\n",
    "print(\"fk: \", cg_fk_x1) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best evaluation with steepest descent method:\n",
    "\n",
    "x1:  [-1.2] [1.]\n",
    "\n",
    "best set of parameters:  {'c': 1e-06, 'rho': 0.7}\n",
    "\n",
    "xk:  [1.] [1.]\n",
    "\n",
    "k:  35285\n",
    "\n",
    "fk:  [4.17283211e-25]\n",
    "\n",
    "\n",
    "Evaluation of the conjugate gradient method using the best parameters of steepest descent: \n",
    "\n",
    "x0:  [-1.2] [1.]\n",
    "\n",
    "set of parameters:  {'c': 1e-06, 'rho': 0.7}\n",
    "\n",
    "xk:  [1.] [1.]\n",
    "\n",
    "k:  8191\n",
    "\n",
    "fk:  [2.04761125e-25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation with conjugate gradient method:\n",
      "x0:  [-1.2] [1.]\n",
      "best set of parameters:  {'c': 0.01, 'rho': 0.4}\n",
      "xk:  [1.] [1.]\n",
      "k:  318\n",
      "fk:  [5.77643398e-28]\n",
      "\n",
      "Evaluation of the steepest descent method using the best parameters of conjugate gradient: \n",
      "x0:  [-1.2] [1.]\n",
      "set of parameters:  {'c': 0.01, 'rho': 0.4}\n",
      "xk:  [1.] [1.]\n",
      "k:  28146\n",
      "fk:  [8.1509053e-25]\n"
     ]
    }
   ],
   "source": [
    "print(\"Best evaluation with conjugate gradient method:\")\n",
    "print(\"x0: \", x1[0], x1[1])\n",
    "print(\"best set of parameters: \", cg_best_param_x1)\n",
    "print(\"xk: \", cg_best_xk_x1[0], cg_best_xk_x1[1])\n",
    "print(\"k: \", cg_best_k_x1)\n",
    "print(\"fk: \", cg_best_fk_x1) \n",
    "\n",
    "print(\"\\nEvaluation of the steepest descent method using the best parameters of conjugate gradient: \")\n",
    "print(\"x0: \", x1[0], x1[1])\n",
    "print(\"set of parameters: \", cg_best_param_x1)\n",
    "print(\"xk: \", sd_xk_x1[0], sd_xk_x1[1])\n",
    "print(\"k: \", sd_k_x1)\n",
    "print(\"fk: \", sd_fk_x1) \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best evaluation with conjugate gradient method:\n",
    "\n",
    "x0:  [-1.2] [1.]\n",
    "\n",
    "best set of parameters:  {'c': 0.01, 'rho': 0.4}\n",
    "\n",
    "xk:  [1.] [1.]\n",
    "\n",
    "k:  318\n",
    "\n",
    "fk:  [5.77643398e-28]\n",
    "\n",
    "\n",
    "Evaluation of the steepest descent method using the best parameters of conjugate gradient: \n",
    "\n",
    "x0:  [-1.2] [1.]\n",
    "\n",
    "set of parameters:  {'c': 0.01, 'rho': 0.4}\n",
    "\n",
    "xk:  [1.] [1.]\n",
    "\n",
    "k:  28146\n",
    "\n",
    "fk:  [8.1509053e-25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of rate of convergence for steepest descent method:\n",
      "p mean = 1.30\n"
     ]
    }
   ],
   "source": [
    "if (sd_best_gradfk_norm_x1 <= tolgrad):\n",
    "    #It make sense to evaluate the rate of convergence if and only if I've reached the solution \n",
    "    p = rate_of_convergence(sd_best_xseq_x1)\n",
    "    print(\"Evaluation of rate of convergence for steepest descent method:\")\n",
    "    print(f\"p mean = {np.abs(p).mean():.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of rate of convergence for steepest descent method:\n",
    "\n",
    "p mean = 1.30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of rate of convergence for conjugate gradient method:\n",
      "p mean = 2.18\n"
     ]
    }
   ],
   "source": [
    "if (cg_best_gradfk_norm_x1 <= tolgrad):\n",
    "    #It make sense to evaluate the rate of convergence if and only if I've reached the solution \n",
    "    p = rate_of_convergence(cg_best_xseq_x1)\n",
    "    print(\"Evaluation of rate of convergence for conjugate gradient method:\")\n",
    "    print(f\"p mean = {np.abs(p).mean():.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of rate of convergence for conjugate gradient method:\n",
    "\n",
    "p mean = 2.18"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
