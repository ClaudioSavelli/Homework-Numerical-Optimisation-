{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import steepest_descent_bcktrck as sdb\n",
    "from steepest_descent_bcktrck import *\n",
    "import cgm_pol_rib as cgmpb\n",
    "from cgm_pol_rib import *\n",
    "import functions as funcs\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'cgm_pol_rib' from '/home/claudio/Documents/GitHub/Homework-Numerical-Optimisation-/Homework Numerical Optimisation/Unconstrained Optimisation/cgm_pol_rib.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(funcs)\n",
    "reload(sdb)\n",
    "reload(cgmpb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate rate of convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_of_convergence(xseq: np.ndarray) ->np.ndarray:\n",
    "    N = 100\n",
    "    n = xseq.shape[1]\n",
    "    p = np.empty(N)\n",
    "    for k in range(1, N+1):\n",
    "        if (xseq[:, k+2]-xseq[:, k+1]).any() != 0 and (xseq[:, k+1]-xseq[:, k]).any() != 0 and (xseq[:, k]-xseq[:, k-1]).any() != 0:\n",
    "            p[k-1] = log(np.linalg.norm(xseq[:, k+2]-xseq[:, k+1], 2) / (np.linalg.norm(xseq[:, k+1]-xseq[:, k], 2))) / \\\n",
    "                     log(np.linalg.norm(xseq[:, k+1]-xseq[:, k], 2) / (np.linalg.norm(xseq[:, k]-xseq[:, k-1], 2)))\n",
    "            # print(f\"i = {k-1}: p = {p}\")\n",
    "    return p"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialisation of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.array([1.2, 1.2]).reshape(-1,1)\n",
    "x1 = np.array([-1.2, 1]).reshape(-1,1)\n",
    "alpha0 = 1\n",
    "tolgrad = 1e-12\n",
    "rho = 0.5\n",
    "c = 1e-4\n",
    "kmax = 40000\n",
    "btmax = 50\n",
    "fin_diff = False\n",
    "fd_type = 'centered'\n",
    "\n",
    "params = {\"c\": [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6],\n",
    "              \"rho\": [0.2, 0.3, 0.4, 0.5, 0.6, 0.7]}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the Steepest descent and conjugate gradient method with Backtrack tuning the parameters c and rho using the Armijo condition to see what is the result obtained (POINT X0)\n",
    "\n",
    "Here our idea is quite simple. In order to make a complete comparison between the two methods the best set of paramenters for both methods is evaluated, once this is done a comparison is made with the best result obtained in one of the two methods with one set and the same set is also used for the other method (we will therefore have in all 2 saved evaluations for steepest descent, one with the best set of parameters, and the other with the best set of parameters of the conjugate gradient method, and 2 saved evaluations for the conjugate gradient method) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best result for steepest descent method is saved here\n",
    "sd_best_fk_x0 = 1\n",
    "sd_best_param_x0 = 0\n",
    "sd_best_k_x0 = 0\n",
    "sd_best_gradfk_norm_x0 = 10\n",
    "sd_best_xk_x0 = np.empty((2,1))\n",
    "\n",
    "#best result for conjugate gradient method is saved here\n",
    "cg_best_fk_x0 = 1\n",
    "cg_best_param_x0 = 0\n",
    "cg_best_k_x0 = 0\n",
    "cg_best_gradfk_norm_x0 = 10\n",
    "cg_best_xk_x0 = np.empty((2,1))\n",
    "\n",
    "#Other result of steepest descent method is saved here\n",
    "sd_fk_x0 = 1\n",
    "sd_param_x0 = 0\n",
    "sd_k_x0 = 0\n",
    "sd_xk_x0 = np.empty((2,1))\n",
    "\n",
    "#Other result of conjugate gradient method is saved here\n",
    "cg_fk_x0 = 1\n",
    "cg_param_x0 = 0\n",
    "cg_k_x0 = 0\n",
    "cg__xk_x0 = np.empty((2,1))\n",
    "\n",
    "for param in ParameterGrid(params): \n",
    "    sd_xk, sd_fk, sd_gradfk_norm, sd_k, sd_xseq, sd_btseq = steepest_descent_bcktrck(x0, 'Rosenbrock', alpha0, kmax, tolgrad, param[\"c\"], param[\"rho\"], btmax, fin_diff, fd_type)\n",
    "\n",
    "    cg_xk, cg_fk, cg_gradfk_norm, cg_k, cg_xseq, cg_btseq = cgm_pol_rib(x0, 'Rosenbrock', alpha0, kmax, tolgrad, param[\"c\"], param[\"rho\"], btmax, fin_diff, fd_type)\n",
    "    \n",
    "    if (sd_fk < sd_best_fk_x0): \n",
    "        sd_best_param_x0 = param\n",
    "\n",
    "        sd_best_xk_x0 = sd_xk\n",
    "        sd_best_k_x0 = sd_k\n",
    "        sd_best_fk_x0 = sd_fk\n",
    "        sd_best_gradfk_norm_x0 = sd_gradfk_norm\n",
    "        sd_best_xseq_x0 = np.copy(sd_xseq)\n",
    "        \n",
    "        cg_xk_x0 = cg_xk\n",
    "        cg_k_x0 = cg_k\n",
    "        cg_fk_x0 = cg_fk\n",
    "            \n",
    "    if (cg_fk < cg_best_fk_x0): \n",
    "        cg_best_param_x0 = param\n",
    "\n",
    "        cg_best_xk_x0 = cg_xk\n",
    "        cg_best_k_x0 = cg_k\n",
    "        cg_best_fk_x0 = cg_fk\n",
    "        cg_best_gradfk_norm_x0 = cg_gradfk_norm\n",
    "        cg_best_xseq_x0 = np.copy(cg_xseq)\n",
    "\n",
    "        \n",
    "        sd_xk_x0 = sd_xk\n",
    "        sd_k_x0 = sd_k\n",
    "        sd_fk_x0 = sd_fk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation with steepest descent method:\n",
      "x0:  [1.2] [1.2]\n",
      "best set of parameters:  {'c': 0.01, 'rho': 0.7}\n",
      "xk:  [1.] [1.]\n",
      "k:  31839\n",
      "fk:  [4.17569518e-25]\n",
      "\n",
      "Evaluation of the conjugate gradient method using the best parameters of steepest descent: \n",
      "x0:  [1.2] [1.2]\n",
      "set of parameters:  0\n",
      "xk:  [1.] [1.]\n",
      "k:  8589\n",
      "fk:  [3.1537594e-25]\n"
     ]
    }
   ],
   "source": [
    "print(\"Best evaluation with steepest descent method:\")\n",
    "print(\"x0: \", x0[0], x0[1])\n",
    "print(\"best set of parameters: \", sd_best_param_x0)\n",
    "print(\"xk: \", sd_best_xk_x0[0], sd_best_xk_x0[1])\n",
    "print(\"k: \", sd_best_k_x0)\n",
    "print(\"fk: \", sd_best_fk_x0) \n",
    "\n",
    "print(\"\\nEvaluation of the conjugate gradient method using the best parameters of steepest descent: \")\n",
    "print(\"x0: \", x0[0], x0[1])\n",
    "print(\"set of parameters: \", sd_best_param_x0)\n",
    "print(\"xk: \", cg_xk_x0[0], cg_xk_x0[1])\n",
    "print(\"k: \", cg_k_x0)\n",
    "print(\"fk: \", cg_fk_x0) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best evaluation with steepest descent method:\n",
    "\n",
    "x0:  [1.2] [1.2]\n",
    "\n",
    "best set of parameters:  {'c': 0.01, 'rho': 0.7}\n",
    "\n",
    "xk:  [1.] [1.]\n",
    "\n",
    "k:  31839\n",
    "\n",
    "fk:  [4.17569518e-25]\n",
    "\n",
    "\n",
    "Evaluation of the conjugate gradient method using the best parameters of steepest descent: \n",
    "\n",
    "x0:  [1.2] [1.2]\n",
    "\n",
    "set of parameters:  {'c': 0.01, 'rho': 0.7}\n",
    "\n",
    "xk:  [1.] [1.]\n",
    "\n",
    "k:  8589\n",
    "\n",
    "fk:  [3.1537594e-25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation with steepest descent method:\n",
      "x0:  [1.2] [1.2]\n",
      "best set of parameters:  {'c': 0.1, 'rho': 0.4}\n",
      "xk:  [1.] [1.]\n",
      "k:  330\n",
      "fk:  [8.72184338e-29]\n",
      "\n",
      "Evaluation of the conjugate gradient method using the best parameters of steepest descent: \n",
      "x0:  [1.2] [1.2]\n",
      "set of parameters:  0\n",
      "xk:  [1.] [1.]\n",
      "k:  27922\n",
      "fk:  [8.37329555e-25]\n"
     ]
    }
   ],
   "source": [
    "print(\"Best evaluation with conjugate gradient method:\")\n",
    "print(\"x0: \", x0[0], x0[1])\n",
    "print(\"best set of parameters: \", cg_best_param_x0)\n",
    "print(\"xk: \", cg_best_xk_x0[0], cg_best_xk_x0[1])\n",
    "print(\"k: \", cg_best_k_x0)\n",
    "print(\"fk: \", cg_best_fk_x0) \n",
    "\n",
    "print(\"\\nEvaluation of the steepest descent method using the best parameters of conjugate gradient: \")\n",
    "print(\"x0: \", x0[0], x0[1])\n",
    "print(\"set of parameters: \", cg_best_param_x0)\n",
    "print(\"xk: \", sd_xk_x0[0], sd_xk_x0[1])\n",
    "print(\"k: \", sd_k_x0)\n",
    "print(\"fk: \", sd_fk_x0) \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best evaluation with conjugate gradient method:\n",
    "\n",
    "x0:  [1.2] [1.2]\n",
    "\n",
    "best set of parameters:  {'c': 0.1, 'rho': 0.4}\n",
    "\n",
    "xk:  [1.] [1.]\n",
    "\n",
    "k:  330\n",
    "\n",
    "fk:  [8.72184338e-29]\n",
    "\n",
    "Evaluation of the steepest descent method using the best parameters of conjugate gradient: \n",
    "\n",
    "x0:  [1.2] [1.2]\n",
    "\n",
    "set of parameters:  {'c': 0.1, 'rho': 0.4}\n",
    "\n",
    "xk:  [1.] [1.]\n",
    "\n",
    "k:  27922\n",
    "\n",
    "fk:  [8.37329555e-25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of rate of convergence for steepest descent method:\n",
      "p mean = 1.13\n"
     ]
    }
   ],
   "source": [
    "if (sd_best_gradfk_norm_x0 <= tolgrad):\n",
    "    #It make sense to evaluate the rate of convergence if and only if I've reached the solution \n",
    "    p = rate_of_convergence(sd_best_xseq_x0)\n",
    "    print(\"Evaluation of rate of convergence for steepest descent method:\")\n",
    "    print(f\"p mean = {np.abs(p).mean():.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of rate of convergence for steepest descent method:\n",
    "\n",
    "p mean = 1.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of rate of convergence for conjugate gradient method:\n",
      "p mean = 3.78\n"
     ]
    }
   ],
   "source": [
    "if (cg_best_gradfk_norm_x0 <= tolgrad):\n",
    "    #It make sense to evaluate the rate of convergence if and only if I've reached the solution \n",
    "    p = rate_of_convergence(cg_best_xseq_x0)\n",
    "    print(\"Evaluation of rate of convergence for conjugate gradient method:\")\n",
    "    print(f\"p mean = {np.abs(p).mean():.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of rate of convergence for conjugate gradient method:\n",
    "\n",
    "p mean = 3.78"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do the same but using another starting point (x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best result for steepest descent method is saved here\n",
    "sd_best_fk_x1 = 1\n",
    "sd_best_param_x1 = 0\n",
    "sd_best_k_x1 = 0\n",
    "sd_best_gradfk_norm_x1 = 10\n",
    "sd_best_xk_x1 = np.empty((2,1))\n",
    "\n",
    "#best result for conjugate gradient method is saved here\n",
    "cg_best_fk_x1 = 1\n",
    "cg_best_param_x1 = 0\n",
    "cg_best_k_x1 = 0\n",
    "cg_best_gradfk_norm_x1 = 10\n",
    "cg_best_xk_x1 = np.empty(2,1)\n",
    "\n",
    "#Other result of steepest descent method is saved here\n",
    "sd_fk_x1 = 1\n",
    "sd_param_x1 = 0\n",
    "sd_k_x1 = 0\n",
    "sd_xk_x1 = np.empty((2,1))\n",
    "\n",
    "#Other result of conjugate gradient method is saved here\n",
    "cg_fk_x1 = 1\n",
    "cg_param_x1 = 0\n",
    "cg_k_x1 = 0\n",
    "cg__xk_x1 = np.empty((2,1))\n",
    "\n",
    "for param in ParameterGrid(params): \n",
    "    sd_xk, sd_fk, sd_gradfk_norm, sd_k, sd_xseq, sd_btseq = steepest_descent_bcktrck(x1, 'Rosenbrock', alpha0, kmax, tolgrad, param[\"c\"], param[\"rho\"], btmax, fin_diff, fd_type)\n",
    "\n",
    "    cg_xk, cg_fk, cg_gradfk_norm, cg_k, cg_xseq, cg_btseq = cgm_pol_rib(x1, 'Rosenbrock', alpha0, kmax, tolgrad, param[\"c\"], param[\"rho\"], btmax, fin_diff, fd_type)\n",
    "    \n",
    "    if (sd_fk < sd_best_fk_x0): \n",
    "        sd_best_param_x1 = param\n",
    "\n",
    "        sd_best_xk_x1 = sd_xk\n",
    "        sd_best_k_x1 = sd_k\n",
    "        sd_best_fk_x1 = sd_fk\n",
    "        sd_best_gradfk_norm_x1 = sd_gradfk_norm\n",
    "        sd_best_xseq_x1 = np.copy(sd_xseq)\n",
    "        \n",
    "        cg_xk_x1 = cg_xk\n",
    "        cg_k_x1 = cg_k\n",
    "        cg_fk_x1 = cg_fk\n",
    "            \n",
    "    if (cg_fk < cg_best_fk_x1): \n",
    "        cg_best_param_x1 = param\n",
    "\n",
    "        cg_best_xk_x1 = cg_xk\n",
    "        cg_best_k_x1 = cg_k\n",
    "        cg_best_fk_x1 = cg_fk\n",
    "        cg_best_gradfk_norm_x1 = cg_gradfk_norm\n",
    "        cg_best_xseq_x1 = np.copy(cg_xseq)\n",
    "\n",
    "        \n",
    "        sd_xk_x1 = sd_xk\n",
    "        sd_k_x1 = sd_k\n",
    "        sd_fk_x1 = sd_fk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation with steepest descent method:\n",
      "x1:  [-1.2] [1.]\n",
      "best set of parameters:  {'c': 1e-06, 'rho': 0.7}\n",
      "xk:  [1.] [1.]\n",
      "k:  35285\n",
      "fk:  [4.17283211e-25]\n",
      "\n",
      "Evaluation of the conjugate gradient method using the best parameters of steepest descent: \n",
      "x0:  [-1.2] [1.]\n",
      "set of parameters:  {'c': 1e-06, 'rho': 0.7}\n",
      "xk:  [1.] [1.]\n",
      "k:  8191\n",
      "fk:  [2.04761125e-25]\n"
     ]
    }
   ],
   "source": [
    "print(\"Best evaluation with steepest descent method:\")\n",
    "print(\"x1: \", x1[0], x1[1])\n",
    "print(\"best set of parameters: \", sd_best_param_x1)\n",
    "print(\"xk: \", sd_best_xk_x1[0], sd_best_xk_x1[1])\n",
    "print(\"k: \", sd_best_k_x1)\n",
    "print(\"fk: \", sd_best_fk_x1) \n",
    "\n",
    "print(\"\\nEvaluation of the conjugate gradient method using the best parameters of steepest descent: \")\n",
    "print(\"x1: \", x1[0], x1[1])\n",
    "print(\"set of parameters: \", sd_best_param_x1)\n",
    "print(\"xk: \", cg_xk_x1[0], cg_xk_x1[1])\n",
    "print(\"k: \", cg_k_x1)\n",
    "print(\"fk: \", cg_fk_x1) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best evaluation with steepest descent method:\n",
    "\n",
    "x1:  [-1.2] [1.]\n",
    "\n",
    "best set of parameters:  {'c': 1e-06, 'rho': 0.7}\n",
    "\n",
    "xk:  [1.] [1.]\n",
    "\n",
    "k:  35285\n",
    "\n",
    "fk:  [4.17283211e-25]\n",
    "\n",
    "\n",
    "Evaluation of the conjugate gradient method using the best parameters of steepest descent: \n",
    "\n",
    "x1:  [-1.2] [1.]\n",
    "\n",
    "set of parameters:  {'c': 1e-06, 'rho': 0.7}\n",
    "\n",
    "xk:  [1.] [1.]\n",
    "\n",
    "k:  8191\n",
    "\n",
    "fk:  [2.04761125e-25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluation with conjugate gradient method:\n",
      "x0:  [-1.2] [1.]\n",
      "best set of parameters:  {'c': 0.01, 'rho': 0.4}\n",
      "xk:  [1.] [1.]\n",
      "k:  318\n",
      "fk:  [5.77643398e-28]\n",
      "\n",
      "Evaluation of the steepest descent method using the best parameters of conjugate gradient: \n",
      "x0:  [-1.2] [1.]\n",
      "set of parameters:  {'c': 0.01, 'rho': 0.4}\n",
      "xk:  [1.] [1.]\n",
      "k:  28146\n",
      "fk:  [8.1509053e-25]\n"
     ]
    }
   ],
   "source": [
    "print(\"Best evaluation with conjugate gradient method:\")\n",
    "print(\"x1: \", x1[0], x1[1])\n",
    "print(\"best set of parameters: \", cg_best_param_x1)\n",
    "print(\"xk: \", cg_best_xk_x1[0], cg_best_xk_x1[1])\n",
    "print(\"k: \", cg_best_k_x1)\n",
    "print(\"fk: \", cg_best_fk_x1) \n",
    "\n",
    "print(\"\\nEvaluation of the steepest descent method using the best parameters of conjugate gradient: \")\n",
    "print(\"x1: \", x1[0], x1[1])\n",
    "print(\"set of parameters: \", cg_best_param_x1)\n",
    "print(\"xk: \", sd_xk_x1[0], sd_xk_x1[1])\n",
    "print(\"k: \", sd_k_x1)\n",
    "print(\"fk: \", sd_fk_x1) \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best evaluation with conjugate gradient method:\n",
    "\n",
    "x1:  [-1.2] [1.]\n",
    "\n",
    "best set of parameters:  {'c': 0.01, 'rho': 0.4}\n",
    "\n",
    "xk:  [1.] [1.]\n",
    "\n",
    "k:  318\n",
    "\n",
    "fk:  [5.77643398e-28]\n",
    "\n",
    "\n",
    "Evaluation of the steepest descent method using the best parameters of conjugate gradient: \n",
    "\n",
    "x1:  [-1.2] [1.]\n",
    "\n",
    "set of parameters:  {'c': 0.01, 'rho': 0.4}\n",
    "\n",
    "xk:  [1.] [1.]\n",
    "\n",
    "k:  28146\n",
    "\n",
    "fk:  [8.1509053e-25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of rate of convergence for steepest descent method:\n",
      "p mean = 1.30\n"
     ]
    }
   ],
   "source": [
    "if (sd_best_gradfk_norm_x1 <= tolgrad):\n",
    "    #It make sense to evaluate the rate of convergence if and only if I've reached the solution \n",
    "    p = rate_of_convergence(sd_best_xseq_x1)\n",
    "    print(\"Evaluation of rate of convergence for steepest descent method:\")\n",
    "    print(f\"p mean = {np.abs(p).mean():.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of rate of convergence for steepest descent method:\n",
    "\n",
    "p mean = 1.30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of rate of convergence for conjugate gradient method:\n",
      "p mean = 2.18\n"
     ]
    }
   ],
   "source": [
    "if (cg_best_gradfk_norm_x1 <= tolgrad):\n",
    "    #It make sense to evaluate the rate of convergence if and only if I've reached the solution \n",
    "    p = rate_of_convergence(cg_best_xseq_x1)\n",
    "    print(\"Evaluation of rate of convergence for conjugate gradient method:\")\n",
    "    print(f\"p mean = {np.abs(p).mean():.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of rate of convergence for conjugate gradient method:\n",
    "\n",
    "p mean = 2.18"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's work with the other functions and collect the data. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Extended Powell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmax = 100\n",
    "x0 = np.resize([3, -1, 0, 1],1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_xk_ep, sd_fk_ep, sd_gradfk_norm_ep, sd_k_ep, sd_xseq_ep, sd_btseq_ep = steepest_descent_bcktrck(x0, 'Extended Powell', alpha0, kmax, tolgrad, c, rho, btmax, fin_diff, fd_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Result of steepest descent method:\")\n",
    "print(\"x0: \", x0, \" (length: \", len(x0), \")\")\n",
    "print(\"k: \", sd_k_ep)\n",
    "print(\"fk: \", sd_fk_ep) \n",
    "print(\"gradfk: \", sd_gradfk_norm_ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cg_xk_ep, cg_fk_ep, cg_gradfk_norm_ep, cg_k_ep, cg_xseq_ep, cg_btseq_ep = cgm_pol_rib(x0, 'Extended Powell', alpha0, kmax, tolgrad, c, rho, btmax, fin_diff, fd_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nResult of conjugate gradient method:\")\n",
    "print(\"x0: \", x0, \" (length: \", len(x0), \")\")\n",
    "print(\"k: \", cg_k_ep)\n",
    "print(\"fk: \", cg_fk_ep) \n",
    "print(\"gradfk: \", cg_gradfk_norm_ep)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Extended Rosenbrook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [36], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m kmax \u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m\n\u001b[1;32m      2\u001b[0m x0 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mresize([\u001b[39m-\u001b[39m\u001b[39m1.2\u001b[39m, \u001b[39m1\u001b[39m],\u001b[39m10000\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m sd_xk_er, sd_fk_er, sd_gradfk_norm_er, sd_k_er, sd_xseq_er, sd_btseq_er \u001b[39m=\u001b[39m steepest_descent_bcktrck(x0, \u001b[39m'\u001b[39;49m\u001b[39mExtended Rosenbrock\u001b[39;49m\u001b[39m'\u001b[39;49m, alpha0, kmax, tolgrad, c, rho, btmax, fin_diff, fd_type)\n\u001b[1;32m      6\u001b[0m cg_xk_er, cg_fk_er, cg_gradfk_norm_er, cg_k_er, cg_xseq_er, cg_btseq_er \u001b[39m=\u001b[39m cgm_pol_rib(x0, \u001b[39m'\u001b[39m\u001b[39mExtended Rosenbrock\u001b[39m\u001b[39m'\u001b[39m, alpha0, kmax, tolgrad, c, rho, btmax, fin_diff, fd_type)\n",
      "File \u001b[0;32m~/Documents/GitHub/Homework-Numerical-Optimisation-/Homework Numerical Optimisation/Unconstrained Optimisation/steepest_descent_bcktrck.py:98\u001b[0m, in \u001b[0;36msteepest_descent_bcktrck\u001b[0;34m(x0, f, alpha0, kmax, tolgrad, c1, rho, btmax, fin_diff, fd_type)\u001b[0m\n\u001b[1;32m     96\u001b[0m fk \u001b[39m=\u001b[39m extnd_rosenb(xk)\n\u001b[1;32m     97\u001b[0m k \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m---> 98\u001b[0m gradfk_norm \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mnorm(grad_extnd_rosenb(xk, fin_diff, fd_type), \u001b[39m2\u001b[39m)\n\u001b[1;32m    100\u001b[0m \u001b[39mwhile\u001b[39;00m k \u001b[39m<\u001b[39m kmax \u001b[39mand\u001b[39;00m gradfk_norm \u001b[39m>\u001b[39m tolgrad:\n\u001b[1;32m    101\u001b[0m     pk \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mgrad_extnd_rosenb(xk, fin_diff, fd_type)\n",
      "File \u001b[0;32m~/Documents/GitHub/Homework-Numerical-Optimisation-/Homework Numerical Optimisation/Unconstrained Optimisation/functions.py:211\u001b[0m, in \u001b[0;36mgrad_extnd_rosenb\u001b[0;34m(x, fin_diff, type)\u001b[0m\n\u001b[1;32m    209\u001b[0m             grad[i] \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m(extnd_rosenb(x\u001b[39m-\u001b[39mh\u001b[39m*\u001b[39me[:, i]\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)) \u001b[39m-\u001b[39m fx) \u001b[39m/\u001b[39m h\n\u001b[1;32m    210\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 211\u001b[0m             grad[i] \u001b[39m=\u001b[39m (extnd_rosenb(x\u001b[39m+\u001b[39;49mh\u001b[39m*\u001b[39;49me[:, i]\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m1\u001b[39;49m)) \u001b[39m-\u001b[39m extnd_rosenb(x\u001b[39m-\u001b[39mh\u001b[39m*\u001b[39me[:, i]\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))) \u001b[39m/\u001b[39m (\u001b[39m2\u001b[39m\u001b[39m*\u001b[39mh)\n\u001b[1;32m    212\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    213\u001b[0m     \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, num):\n",
      "File \u001b[0;32m~/Documents/GitHub/Homework-Numerical-Optimisation-/Homework Numerical Optimisation/Unconstrained Optimisation/functions.py:181\u001b[0m, in \u001b[0;36mextnd_rosenb\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    179\u001b[0m z \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty((num, \u001b[39m1\u001b[39m))\n\u001b[1;32m    180\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, num):\n\u001b[0;32m--> 181\u001b[0m     z[k] \u001b[39m=\u001b[39m f(x, k\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m    182\u001b[0m \u001b[39mreturn\u001b[39;00m (\u001b[39m0.5\u001b[39m \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39msum(z\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m))[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "kmax = 1000\n",
    "x0 = np.resize([-1.2, 1],10000)\n",
    "\n",
    "sd_xk_er, sd_fk_er, sd_gradfk_norm_er, sd_k_er, sd_xseq_er, sd_btseq_er = steepest_descent_bcktrck(x0, 'Extended Rosenbrock', alpha0, kmax, tolgrad, c, rho, btmax, fin_diff, fd_type)\n",
    "\n",
    "cg_xk_er, cg_fk_er, cg_gradfk_norm_er, cg_k_er, cg_xseq_er, cg_btseq_er = cgm_pol_rib(x0, 'Extended Rosenbrock', alpha0, kmax, tolgrad, c, rho, btmax, fin_diff, fd_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Result of steepest descent method:\")\n",
    "print(\"x0: \", x0, \" (length: \", len(x0), \")\")\n",
    "print(\"k: \", sd_k_er)\n",
    "print(\"fk: \", sd_fk_er) \n",
    "print(\"gradfk: \", sd_gradfk_norm_er)\n",
    "\n",
    "print(\"\\nResult of conjugate gradient method:\")\n",
    "print(\"x0: \", x0, \" (length: \", len(x0), \")\")\n",
    "print(\"k: \", cg_k_er)\n",
    "print(\"fk: \", cg_fk_er) \n",
    "print(\"gradfk: \", cg_gradfk_norm_er)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Banded Trigonometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmax = 1000\n",
    "x0 = np.resize([1],10000)\n",
    "\n",
    "sd_xk_bt, sd_fk_bt, sd_gradfk_norm_bt, sd_k_bt, sd_xseq_bt, sd_btseq_bt = steepest_descent_bcktrck(x0, 'Banded Trigonometric', alpha0, kmax, tolgrad, c, rho, btmax, fin_diff, fd_type)\n",
    "\n",
    "cg_xk_bt, cg_fk_bt, cg_gradfk_norm_bt, cg_k_bt, cg_xseq_bt, cg_btseq_bt = cgm_pol_rib(x0, 'Banded Trigonometric', alpha0, kmax, tolgrad, c, rho, btmax, fin_diff, fd_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Result of steepest descent method:\")\n",
    "print(\"x0: \", x0, \" (length: \", len(x0), \")\")\n",
    "print(\"k: \", sd_k_bt)\n",
    "print(\"fk: \", sd_fk_bt) \n",
    "print(\"gradfk: \", sd_gradfk_norm_bt)\n",
    "\n",
    "print(\"\\nResult of conjugate gradient method:\")\n",
    "print(\"x0: \", x0, \" (length: \", len(x0), \")\")\n",
    "print(\"k: \", cg_k_bt)\n",
    "print(\"fk: \", cg_fk_bt) \n",
    "print(\"gradfk: \", cg_gradfk_norm_bt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
