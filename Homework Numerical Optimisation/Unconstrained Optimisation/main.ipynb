{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import steepest_descent_bcktrck as sdb\n",
    "from steepest_descent_bcktrck import *\n",
    "import cgm_pol_rib as cgmpb\n",
    "from cgm_pol_rib import *\n",
    "import functions as funcs\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(funcs)\n",
    "reload(sdb)\n",
    "reload(cgmpb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate rate of convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_of_convergence(x_seq: np.ndarray) ->np.ndarray:\n",
    "    N = 100\n",
    "    n = x_seq.shape[1]\n",
    "    p = np.empty(N)\n",
    "    for k in range(1, N+1):\n",
    "        if (x_seq[k+2, :] - x_seq[k+1, :]).any() != 0 and (x_seq[k+1, :] - x_seq[k, :]).any() != 0 and (x_seq[k, :] - x_seq[k-1, :]).any() != 0:\n",
    "            p[k-1] = log(np.linalg.norm(x_seq[k+2, :] - x_seq[k+1, :], 2) / (np.linalg.norm(x_seq[k+1, :] - x_seq[k, :], 2))) / \\\n",
    "                     log(np.linalg.norm(x_seq[k+1, :] - x_seq[k, :], 2) / (np.linalg.norm(x_seq[k, :] - x_seq[k-1, :], 2)))\n",
    "            # print(f\"i = {k-1}: p = {p}\")\n",
    "    return p"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialisation of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.array([1.2, 1.2])\n",
    "x1 = np.array([-1.2, 1])\n",
    "alpha0 = 1\n",
    "tolgrad = 1e-12\n",
    "rho = 0.5\n",
    "c = 1e-4\n",
    "kmax = 5000\n",
    "btmax = 50\n",
    "fin_diff = False\n",
    "fd_type = 'centered'\n",
    "\n",
    "params = {\"c\": [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6],\n",
    "              \"rho\": [0.2, 0.3, 0.4, 0.5, 0.6, 0.7]}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the Steepest descent and conjugate gradient method with Backtrack tuning the parameters c and rho using the Armijo condition to see what is the result obtained (POINT X0)\n",
    "\n",
    "Here our idea is quite simple. In order to make a complete comparison between the two methods the best set of paramenters for both methods is evaluated, once this is done a comparison is made with the best result obtained in one of the two methods with one set and the same set is also used for the other method (we will therefore have in all 2 saved evaluations for steepest descent, one with the best set of parameters, and the other with the best set of parameters of the conjugate gradient method, and 2 saved evaluations for the conjugate gradient method) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best result for steepest descent method is saved here\n",
    "sd_best_fk_x0 = 1\n",
    "sd_best_param_x0 = 0\n",
    "sd_best_k_x0 = 0\n",
    "sd_best_gradfk_norm_x0 = 10\n",
    "sd_best_xk_x0 = np.empty(2)\n",
    "\n",
    "#best result for conjugate gradient method is saved here\n",
    "cg_best_fk_x0 = 1\n",
    "cg_best_param_x0 = 0\n",
    "cg_best_k_x0 = 0\n",
    "cg_best_gradfk_norm_x0 = 10\n",
    "cg_best_xk_x0 = np.empty(2)\n",
    "\n",
    "#Other result of steepest descent method is saved here\n",
    "sd_fk_x0 = 1\n",
    "sd_param_x0 = 0\n",
    "sd_k_x0 = 0\n",
    "sd_xk_x0 = np.empty(2)\n",
    "\n",
    "#Other result of conjugate gradient method is saved here\n",
    "cg_fk_x0 = 1\n",
    "cg_param_x0 = 0\n",
    "cg_k_x0 = 0\n",
    "cg__xk_x0 = np.empty(2)\n",
    "\n",
    "for param in ParameterGrid(params): \n",
    "    sd_xk, sd_fk, sd_gradfk_norm, sd_k, sd_x_seq, sd_bt_seq = steepest_descent_bcktrck(x0, 'Rosenbrock', alpha0, kmax, tolgrad, param[\"c\"], param[\"rho\"], btmax, fin_diff, fd_type)\n",
    "\n",
    "    cg_xk, cg_fk, cg_gradfk_norm, cg_k, cg_x_seq, cg_bt_seq = cgm_pol_rib(x0, 'Rosenbrock', alpha0, kmax, tolgrad, param[\"c\"], param[\"rho\"], btmax, fin_diff, fd_type)\n",
    "    \n",
    "    if (sd_fk < sd_best_fk_x0): \n",
    "        sd_best_param_x0 = param\n",
    "\n",
    "        sd_best_xk_x0 = sd_xk\n",
    "        sd_best_k_x0 = sd_k\n",
    "        sd_best_fk_x0 = sd_fk\n",
    "        sd_best_gradfk_norm_x0 = sd_gradfk_norm\n",
    "        sd_best_x_seq_x0 = np.copy(sd_x_seq)\n",
    "        \n",
    "        cg_xk_x0 = cg_xk\n",
    "        cg_k_x0 = cg_k\n",
    "        cg_fk_x0 = cg_fk\n",
    "            \n",
    "    if (cg_fk < cg_best_fk_x0): \n",
    "        cg_best_param_x0 = param\n",
    "\n",
    "        cg_best_xk_x0 = cg_xk\n",
    "        cg_best_k_x0 = cg_k\n",
    "        cg_best_fk_x0 = cg_fk\n",
    "        cg_best_gradfk_norm_x0 = cg_gradfk_norm\n",
    "        cg_best_x_seq_x0 = np.copy(cg_x_seq)\n",
    "\n",
    "        \n",
    "        sd_xk_x0 = sd_xk\n",
    "        sd_k_x0 = sd_k\n",
    "        sd_fk_x0 = sd_fk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best evaluation with steepest descent method:\")\n",
    "print(\"x0: \", x0[0], x0[1])\n",
    "print(\"best set of parameters: \", sd_best_param_x0)\n",
    "print(\"xk: \", sd_best_xk_x0[0], sd_best_xk_x0[1])\n",
    "print(\"k: \", sd_best_k_x0)\n",
    "print(\"fk: \", sd_best_fk_x0) \n",
    "\n",
    "print(\"\\nEvaluation of the conjugate gradient method using the best parameters of steepest descent: \")\n",
    "print(\"x0: \", x0[0], x0[1])\n",
    "print(\"set of parameters: \", sd_best_param_x0)\n",
    "print(\"xk: \", cg_xk_x0[0], cg_xk_x0[1])\n",
    "print(\"k: \", cg_k_x0)\n",
    "print(\"fk: \", cg_fk_x0) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best evaluation with steepest descent method:\n",
    "\n",
    "x0:  [1.2] [1.2]\n",
    "\n",
    "best set of parameters:  {'c': 0.01, 'rho': 0.7}\n",
    "\n",
    "xk:  [1.] [1.]\n",
    "\n",
    "k:  31839\n",
    "\n",
    "fk:  [4.17569518e-25]\n",
    "\n",
    "\n",
    "Evaluation of the conjugate gradient method using the best parameters of steepest descent: \n",
    "\n",
    "x0:  [1.2] [1.2]\n",
    "\n",
    "set of parameters:  {'c': 0.01, 'rho': 0.7}\n",
    "\n",
    "xk:  [1.] [1.]\n",
    "\n",
    "k:  8589\n",
    "\n",
    "fk:  [3.1537594e-25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best evaluation with conjugate gradient method:\")\n",
    "print(\"x0: \", x0[0], x0[1])\n",
    "print(\"best set of parameters: \", cg_best_param_x0)\n",
    "print(\"xk: \", cg_best_xk_x0[0], cg_best_xk_x0[1])\n",
    "print(\"k: \", cg_best_k_x0)\n",
    "print(\"fk: \", cg_best_fk_x0) \n",
    "\n",
    "print(\"\\nEvaluation of the steepest descent method using the best parameters of conjugate gradient: \")\n",
    "print(\"x0: \", x0[0], x0[1])\n",
    "print(\"set of parameters: \", cg_best_param_x0)\n",
    "print(\"xk: \", sd_xk_x0[0], sd_xk_x0[1])\n",
    "print(\"k: \", sd_k_x0)\n",
    "print(\"fk: \", sd_fk_x0) \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best evaluation with conjugate gradient method:\n",
    "\n",
    "x0:  [1.2] [1.2]\n",
    "\n",
    "best set of parameters:  {'c': 0.1, 'rho': 0.4}\n",
    "\n",
    "xk:  [1.] [1.]\n",
    "\n",
    "k:  330\n",
    "\n",
    "fk:  [8.72184338e-29]\n",
    "\n",
    "Evaluation of the steepest descent method using the best parameters of conjugate gradient: \n",
    "\n",
    "x0:  [1.2] [1.2]\n",
    "\n",
    "set of parameters:  {'c': 0.1, 'rho': 0.4}\n",
    "\n",
    "xk:  [1.] [1.]\n",
    "\n",
    "k:  27922\n",
    "\n",
    "fk:  [8.37329555e-25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (sd_best_gradfk_norm_x0 <= tolgrad):\n",
    "    #It make sense to evaluate the rate of convergence if and only if I've reached the solution \n",
    "    p = rate_of_convergence(sd_best_x_seq_x0)\n",
    "    print(\"Evaluation of rate of convergence for steepest descent method:\")\n",
    "    print(f\"p mean = {np.abs(p).mean():.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of rate of convergence for steepest descent method:\n",
    "\n",
    "p mean = 1.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (cg_best_gradfk_norm_x0 <= tolgrad):\n",
    "    #It make sense to evaluate the rate of convergence if and only if I've reached the solution \n",
    "    p = rate_of_convergence(cg_best_x_seq_x0)\n",
    "    print(\"Evaluation of rate of convergence for conjugate gradient method:\")\n",
    "    print(f\"p mean = {np.abs(p).mean():.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of rate of convergence for conjugate gradient method:\n",
    "\n",
    "p mean = 3.78"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do the same but using another starting point (x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best result for steepest descent method is saved here\n",
    "sd_best_fk_x1 = 1\n",
    "sd_best_param_x1 = 0\n",
    "sd_best_k_x1 = 0\n",
    "sd_best_gradfk_norm_x1 = 10\n",
    "sd_best_xk_x1 = np.empty(2)\n",
    "\n",
    "#best result for conjugate gradient method is saved here\n",
    "cg_best_fk_x1 = 1\n",
    "cg_best_param_x1 = 0\n",
    "cg_best_k_x1 = 0\n",
    "cg_best_gradfk_norm_x1 = 10\n",
    "cg_best_xk_x1 = np.empty(2)\n",
    "\n",
    "#Other result of steepest descent method is saved here\n",
    "sd_fk_x1 = 1\n",
    "sd_param_x1 = 0\n",
    "sd_k_x1 = 0\n",
    "sd_xk_x1 = np.empty(2)\n",
    "\n",
    "#Other result of conjugate gradient method is saved here\n",
    "cg_fk_x1 = 1\n",
    "cg_param_x1 = 0\n",
    "cg_k_x1 = 0\n",
    "cg__xk_x1 = np.empty(2)\n",
    "\n",
    "for param in ParameterGrid(params): \n",
    "    sd_xk, sd_fk, sd_gradfk_norm, sd_k, sd_x_seq, sd_bt_seq = steepest_descent_bcktrck(x1, 'Rosenbrock', alpha0, kmax, tolgrad, param[\"c\"], param[\"rho\"], btmax, fin_diff, fd_type)\n",
    "\n",
    "    cg_xk, cg_fk, cg_gradfk_norm, cg_k, cg_x_seq, cg_bt_seq = cgm_pol_rib(x1, 'Rosenbrock', alpha0, kmax, tolgrad, param[\"c\"], param[\"rho\"], btmax, fin_diff, fd_type)\n",
    "    \n",
    "    if (sd_fk < sd_best_fk_x0): \n",
    "        sd_best_param_x1 = param\n",
    "\n",
    "        sd_best_xk_x1 = sd_xk\n",
    "        sd_best_k_x1 = sd_k\n",
    "        sd_best_fk_x1 = sd_fk\n",
    "        sd_best_gradfk_norm_x1 = sd_gradfk_norm\n",
    "        sd_best_x_seq_x1 = np.copy(sd_x_seq)\n",
    "        \n",
    "        cg_xk_x1 = cg_xk\n",
    "        cg_k_x1 = cg_k\n",
    "        cg_fk_x1 = cg_fk\n",
    "            \n",
    "    if (cg_fk < cg_best_fk_x1): \n",
    "        cg_best_param_x1 = param\n",
    "\n",
    "        cg_best_xk_x1 = cg_xk\n",
    "        cg_best_k_x1 = cg_k\n",
    "        cg_best_fk_x1 = cg_fk\n",
    "        cg_best_gradfk_norm_x1 = cg_gradfk_norm\n",
    "        cg_best_x_seq_x1 = np.copy(cg_x_seq)\n",
    "\n",
    "        \n",
    "        sd_xk_x1 = sd_xk\n",
    "        sd_k_x1 = sd_k\n",
    "        sd_fk_x1 = sd_fk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best evaluation with steepest descent method:\")\n",
    "print(\"x1: \", x1[0], x1[1])\n",
    "print(\"best set of parameters: \", sd_best_param_x1)\n",
    "print(\"xk: \", sd_best_xk_x1[0], sd_best_xk_x1[1])\n",
    "print(\"k: \", sd_best_k_x1)\n",
    "print(\"fk: \", sd_best_fk_x1) \n",
    "\n",
    "print(\"\\nEvaluation of the conjugate gradient method using the best parameters of steepest descent: \")\n",
    "print(\"x1: \", x1[0], x1[1])\n",
    "print(\"set of parameters: \", sd_best_param_x1)\n",
    "print(\"xk: \", cg_xk_x1[0], cg_xk_x1[1])\n",
    "print(\"k: \", cg_k_x1)\n",
    "print(\"fk: \", cg_fk_x1) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best evaluation with steepest descent method:\n",
    "\n",
    "x1:  [-1.2] [1.]\n",
    "\n",
    "best set of parameters:  {'c': 1e-06, 'rho': 0.7}\n",
    "\n",
    "xk:  [1.] [1.]\n",
    "\n",
    "k:  35285\n",
    "\n",
    "fk:  [4.17283211e-25]\n",
    "\n",
    "\n",
    "Evaluation of the conjugate gradient method using the best parameters of steepest descent: \n",
    "\n",
    "x1:  [-1.2] [1.]\n",
    "\n",
    "set of parameters:  {'c': 1e-06, 'rho': 0.7}\n",
    "\n",
    "xk:  [1.] [1.]\n",
    "\n",
    "k:  8191\n",
    "\n",
    "fk:  [2.04761125e-25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best evaluation with conjugate gradient method:\")\n",
    "print(\"x1: \", x1[0], x1[1])\n",
    "print(\"best set of parameters: \", cg_best_param_x1)\n",
    "print(\"xk: \", cg_best_xk_x1[0], cg_best_xk_x1[1])\n",
    "print(\"k: \", cg_best_k_x1)\n",
    "print(\"fk: \", cg_best_fk_x1) \n",
    "\n",
    "print(\"\\nEvaluation of the steepest descent method using the best parameters of conjugate gradient: \")\n",
    "print(\"x1: \", x1[0], x1[1])\n",
    "print(\"set of parameters: \", cg_best_param_x1)\n",
    "print(\"xk: \", sd_xk_x1[0], sd_xk_x1[1])\n",
    "print(\"k: \", sd_k_x1)\n",
    "print(\"fk: \", sd_fk_x1) \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best evaluation with conjugate gradient method:\n",
    "\n",
    "x1:  [-1.2] [1.]\n",
    "\n",
    "best set of parameters:  {'c': 0.01, 'rho': 0.4}\n",
    "\n",
    "xk:  [1.] [1.]\n",
    "\n",
    "k:  318\n",
    "\n",
    "fk:  [5.77643398e-28]\n",
    "\n",
    "\n",
    "Evaluation of the steepest descent method using the best parameters of conjugate gradient: \n",
    "\n",
    "x1:  [-1.2] [1.]\n",
    "\n",
    "set of parameters:  {'c': 0.01, 'rho': 0.4}\n",
    "\n",
    "xk:  [1.] [1.]\n",
    "\n",
    "k:  28146\n",
    "\n",
    "fk:  [8.1509053e-25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (sd_best_gradfk_norm_x1 <= tolgrad):\n",
    "    #It make sense to evaluate the rate of convergence if and only if I've reached the solution \n",
    "    p = rate_of_convergence(sd_best_x_seq_x1)\n",
    "    print(\"Evaluation of rate of convergence for steepest descent method:\")\n",
    "    print(f\"p mean = {np.abs(p).mean():.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of rate of convergence for steepest descent method:\n",
    "\n",
    "p mean = 1.30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (cg_best_gradfk_norm_x1 <= tolgrad):\n",
    "    #It make sense to evaluate the rate of convergence if and only if I've reached the solution \n",
    "    p = rate_of_convergence(cg_best_x_seq_x1)\n",
    "    print(\"Evaluation of rate of convergence for conjugate gradient method:\")\n",
    "    print(f\"p mean = {np.abs(p).mean():.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of rate of convergence for conjugate gradient method:\n",
    "\n",
    "p mean = 2.18"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's work with the other functions and collect the data. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Extended Powell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmax = 5000\n",
    "\n",
    "x0_array = np.random.randint(1,10,size = (10,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmean = 0 \n",
    "grad_norm_mean = 0\n",
    "grad_norm_min = 99999999999\n",
    "grad_norm_max = -1\n",
    "fx_mean = 0\n",
    "fx_min = 99999999999\n",
    "fx_max = -1\n",
    "\n",
    "for point in x0_array: \n",
    "    sd_xk_ep, sd_fk_ep, sd_gradfk_norm_ep, sd_k_ep, sd_x_seq_ep, sd_bt_seq_ep = steepest_descent_bcktrck(point, 'Extended Powell', alpha0, kmax, tolgrad, c, rho, btmax, fin_diff, fd_type)\n",
    "    print(\"Result of steepest descent method:\")\n",
    "    print(\"x0: \", point, \" (length: \", len(point), \")\")\n",
    "    print(\"k: \", sd_k_ep)\n",
    "    print(\"fk: \", sd_fk_ep[-1]) \n",
    "    print(\"gradfk: \", sd_gradfk_norm_ep[-1])\n",
    "    print(\"\\n\")\n",
    "\n",
    "    kmean += sd_k_ep\n",
    "    grad_norm_mean += sd_gradfk_norm_ep[-1]\n",
    "    if grad_norm_max < sd_gradfk_norm_ep[-1]: \n",
    "        grad_norm_max = sd_gradfk_norm_ep[-1]\n",
    "    if grad_norm_min > sd_gradfk_norm_ep[-1]: \n",
    "        grad_norm_min = sd_gradfk_norm_ep[-1]\n",
    "    fx_mean += sd_fk_ep[-1]\n",
    "    if fx_max < sd_fk_ep[-1]: \n",
    "        fx_max = sd_fk_ep[-1]\n",
    "    if fx_min > sd_fk_ep[-1]: \n",
    "        fx_min = sd_fk_ep[-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmean = kmean / len(x0_array)\n",
    "grad_norm_mean = grad_norm_mean / len(x0_array)\n",
    "fx_mean = fx_mean / len(x0_array)\n",
    "\n",
    "print(\"mean_of_k: \", kmean)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"min_of_grad_norm: \", grad_norm_min)\n",
    "print(\"mean_of_grad_norm: \", grad_norm_mean)\n",
    "print(\"max_of_grad_norm: \", grad_norm_max)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"min_of_fx: \", fx_min)\n",
    "print(\"mean_of_fx: \", fx_mean)\n",
    "print(\"max_of_fx: \", fx_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmean = 0 \n",
    "grad_norm_mean = 0\n",
    "grad_norm_min = 99999999999\n",
    "grad_norm_max = -1\n",
    "fx_mean = 0\n",
    "fx_min = 99999999999\n",
    "fx_max = -1\n",
    "\n",
    "for point in x0_array: \n",
    "    sd_xk_ep, sd_fk_ep, sd_gradfk_norm_ep, sd_k_ep, sd_x_seq_ep, sd_bt_seq_ep = cgm_pol_rib(point, 'Extended Powell', alpha0, kmax, tolgrad, c, rho, btmax, fin_diff, fd_type)\n",
    "    print(\"Result of conjugate gradient method:\")\n",
    "    print(\"x0: \", point, \" (length: \", len(point), \")\")\n",
    "    print(\"k: \", sd_k_ep)\n",
    "    print(\"fk: \", sd_fk_ep[-1]) \n",
    "    print(\"gradfk: \", sd_gradfk_norm_ep[-1])\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    kmean += sd_k_ep\n",
    "    grad_norm_mean += sd_gradfk_norm_ep[-1]\n",
    "    if grad_norm_max < sd_gradfk_norm_ep[-1]: \n",
    "        grad_norm_max = sd_gradfk_norm_ep[-1]\n",
    "    if grad_norm_min > sd_gradfk_norm_ep[-1]: \n",
    "        grad_norm_min = sd_gradfk_norm_ep[-1]\n",
    "    fx_mean += sd_fk_ep[-1]\n",
    "    if fx_max < sd_fk_ep[-1]: \n",
    "        fx_max = sd_fk_ep[-1]\n",
    "    if fx_min > sd_fk_ep[-1]: \n",
    "        fx_min = sd_fk_ep[-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmean = kmean / len(x0_array)\n",
    "grad_norm_mean = grad_norm_mean / len(x0_array)\n",
    "fx_mean = fx_mean / len(x0_array)\n",
    "\n",
    "print(\"mean_of_k: \", kmean)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"min_of_grad_norm: \", grad_norm_min)\n",
    "print(\"mean_of_grad_norm: \", grad_norm_mean)\n",
    "print(\"max_of_grad_norm: \", grad_norm_max)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"min_of_fx: \", fx_min)\n",
    "print(\"mean_of_fx: \", fx_mean)\n",
    "print(\"max_of_fx: \", fx_max)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Extended Rosenbrook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmean = 0 \n",
    "grad_norm_mean = 0\n",
    "grad_norm_min = 99999999999\n",
    "grad_norm_max = -1\n",
    "fx_mean = 0\n",
    "fx_min = 99999999999\n",
    "fx_max = -1\n",
    "\n",
    "for point in x0_array: \n",
    "    sd_xk_er, sd_fk_er, sd_gradfk_norm_er, sd_k_er, sd_x_seq_er, sd_bt_seq_er = steepest_descent_bcktrck(point, 'Extended Rosenbrook', alpha0, kmax, tolgrad, c, rho, btmax, fin_diff, fd_type)\n",
    "    print(\"Result of steepest descent method:\")\n",
    "    print(\"x0: \", point, \" (length: \", len(point), \")\")\n",
    "    print(\"k: \", sd_k_er)\n",
    "    print(\"fk: \", sd_fk_er[-1]) \n",
    "    print(\"gradfk: \", sd_gradfk_norm_er[-1])\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    kmean += sd_k_er\n",
    "    grad_norm_mean += sd_gradfk_norm_er[-1]\n",
    "    if grad_norm_max < sd_gradfk_norm_er[-1]: \n",
    "        grad_norm_max = sd_gradfk_norm_er[-1]\n",
    "    if grad_norm_min > sd_gradfk_norm_er[-1]: \n",
    "        grad_norm_min = sd_gradfk_norm_er[-1]\n",
    "    fx_mean += sd_fk_er[-1]\n",
    "    if fx_max < sd_fk_er[-1]: \n",
    "        fx_max = sd_fk_er[-1]\n",
    "    if fx_min > sd_fk_er[-1]: \n",
    "        fx_min = sd_fk_er[-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmean = kmean / len(x0_array)\n",
    "grad_norm_mean = grad_norm_mean / len(x0_array)\n",
    "fx_mean = fx_mean / len(x0_array)\n",
    "\n",
    "print(\"mean_of_k: \", kmean)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"min_of_grad_norm: \", grad_norm_min)\n",
    "print(\"mean_of_grad_norm: \", grad_norm_mean)\n",
    "print(\"max_of_grad_norm: \", grad_norm_max)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"min_of_fx: \", fx_min)\n",
    "print(\"mean_of_fx: \", fx_mean)\n",
    "print(\"max_of_fx: \", fx_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmean = 0 \n",
    "grad_norm_mean = 0\n",
    "grad_norm_min = 99999999999\n",
    "grad_norm_max = -1\n",
    "fx_mean = 0\n",
    "fx_min = 99999999999\n",
    "fx_max = -1\n",
    "\n",
    "for point in x0_array: \n",
    "    sd_xk_er, sd_fk_er, sd_gradfk_norm_er, sd_k_er, sd_x_seq_er, sd_bt_seq_er = cgm_pol_rib(point, 'Extended Rosenbrook', alpha0, kmax, tolgrad, c, rho, btmax, fin_diff, fd_type)\n",
    "    print(\"Result of conjugate gradient method:\")\n",
    "    print(\"x0: \", point, \" (length: \", len(point), \")\")\n",
    "    print(\"k: \", sd_k_er)\n",
    "    print(\"fk: \", sd_fk_er[-1]) \n",
    "    print(\"gradfk: \", sd_gradfk_norm_er[-1])\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    kmean += sd_k_er\n",
    "    grad_norm_mean += sd_gradfk_norm_er[-1]\n",
    "    if grad_norm_max < sd_gradfk_norm_er[-1]: \n",
    "        grad_norm_max = sd_gradfk_norm_er[-1]\n",
    "    if grad_norm_min > sd_gradfk_norm_er[-1]: \n",
    "        grad_norm_min = sd_gradfk_norm_er[-1]\n",
    "    fx_mean += sd_fk_er[-1]\n",
    "    if fx_max < sd_fk_er[-1]: \n",
    "        fx_max = sd_fk_er[-1]\n",
    "    if fx_min > sd_fk_er[-1]: \n",
    "        fx_min = sd_fk_er[-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmean = kmean / len(x0_array)\n",
    "grad_norm_mean = grad_norm_mean / len(x0_array)\n",
    "fx_mean = fx_mean / len(x0_array)\n",
    "\n",
    "print(\"mean_of_k: \", kmean)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"min_of_grad_norm: \", grad_norm_min)\n",
    "print(\"mean_of_grad_norm: \", grad_norm_mean)\n",
    "print(\"max_of_grad_norm: \", grad_norm_max)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"min_of_fx: \", fx_min)\n",
    "print(\"mean_of_fx: \", fx_mean)\n",
    "print(\"max_of_fx: \", fx_max)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Banded Trigonometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmean = 0 \n",
    "grad_norm_mean = 0\n",
    "grad_norm_min = 99999999999\n",
    "grad_norm_max = -1\n",
    "fx_mean = 0\n",
    "fx_min = 99999999999\n",
    "fx_max = -1\n",
    "\n",
    "for point in x0_array: \n",
    "    sd_xk_bt, sd_fk_bt, sd_gradfk_norm_bt, sd_k_bt, sd_x_seq_bt, sd_bt_seq_bt = steepest_descent_bcktrck(point, 'Banded Trigonometric', alpha0, kmax, tolgrad, c, rho, btmax, fin_diff, fd_type)\n",
    "    print(\"Result of steepest descent method:\")\n",
    "    print(\"x0: \", point, \" (length: \", len(point), \")\")\n",
    "    print(\"k: \", sd_k_bt)\n",
    "    print(\"fk: \", sd_fk_bt[-1]) \n",
    "    print(\"gradfk: \", sd_gradfk_norm_bt[-1])\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    kmean += sd_k_bt\n",
    "    grad_norm_mean += sd_gradfk_norm_bt[-1]\n",
    "    if grad_norm_max < sd_gradfk_norm_bt[-1]: \n",
    "        grad_norm_max = sd_gradfk_norm_bt[-1]\n",
    "    if grad_norm_min > sd_gradfk_norm_bt[-1]: \n",
    "        grad_norm_min = sd_gradfk_norm_bt[-1]\n",
    "    fx_mean += sd_fk_bt[-1]\n",
    "    if fx_max < sd_fk_bt[-1]: \n",
    "        fx_max = sd_fk_bt[-1]\n",
    "    if fx_min > sd_fk_bt[-1]: \n",
    "        fx_min = sd_fk_bt[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmean = kmean / len(x0_array)\n",
    "grad_norm_mean = grad_norm_mean / len(x0_array)\n",
    "fx_mean = fx_mean / len(x0_array)\n",
    "\n",
    "print(\"mean_of_k: \", kmean)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"min_of_grad_norm: \", grad_norm_min)\n",
    "print(\"mean_of_grad_norm: \", grad_norm_mean)\n",
    "print(\"max_of_grad_norm: \", grad_norm_max)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"min_of_fx: \", fx_min)\n",
    "print(\"mean_of_fx: \", fx_mean)\n",
    "print(\"max_of_fx: \", fx_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmean = 0 \n",
    "grad_norm_mean = 0\n",
    "grad_norm_min = 99999999999\n",
    "grad_norm_max = -1\n",
    "fx_mean = 0\n",
    "fx_min = 99999999999\n",
    "fx_max = -1\n",
    "\n",
    "for point in x0_array: \n",
    "    sd_xk_bt, sd_fk_bt, sd_gradfk_norm_bt, sd_k_bt, sd_x_seq_bt, sd_bt_seq_bt = cgm_pol_rib(point, 'Banded Trigonometric', alpha0, kmax, tolgrad, c, rho, btmax, fin_diff, fd_type)\n",
    "    print(\"Result of steepest descent method:\")\n",
    "    print(\"x0: \", point, \" (length: \", len(point), \")\")\n",
    "    print(\"k: \", sd_k_bt)\n",
    "    print(\"fk: \", sd_fk_bt[-1]) \n",
    "    print(\"gradfk: \", sd_gradfk_norm_bt[-1])\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    kmean += sd_k_bt\n",
    "    grad_norm_mean += sd_gradfk_norm_bt[-1]\n",
    "    if grad_norm_max < sd_gradfk_norm_bt[-1]: \n",
    "        grad_norm_max = sd_gradfk_norm_bt[-1]\n",
    "    if grad_norm_min > sd_gradfk_norm_bt[-1]: \n",
    "        grad_norm_min = sd_gradfk_norm_bt[-1]\n",
    "    fx_mean += sd_fk_bt[-1]\n",
    "    if fx_max < sd_fk_bt[-1]: \n",
    "        fx_max = sd_fk_bt[-1]\n",
    "    if fx_min > sd_fk_bt[-1]: \n",
    "        fx_min = sd_fk_bt[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmean = kmean / len(x0_array)\n",
    "grad_norm_mean = grad_norm_mean / len(x0_array)\n",
    "fx_mean = fx_mean / len(x0_array)\n",
    "\n",
    "print(\"mean_of_k: \", kmean)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"min_of_grad_norm: \", grad_norm_min)\n",
    "print(\"mean_of_grad_norm: \", grad_norm_mean)\n",
    "print(\"max_of_grad_norm: \", grad_norm_max)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"min_of_fx: \", fx_min)\n",
    "print(\"mean_of_fx: \", fx_mean)\n",
    "print(\"max_of_fx: \", fx_max)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
